{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SemiBoost implementation, as adapted by Alexandra Sklokin, from papabloblo on github.\n",
    "\n",
    "#### References: \n",
    "\n",
    "papabloblo, SemiBoost, (2018), GitHub repository, https://github.com/papabloblo/semi_boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import sparse\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "\n",
    "class SemiBoostClassifier():\n",
    "\n",
    "    #Note that convergence of the model may depend on the BaseModel. This is the model which will be boosted.\n",
    "    def __init__(self, base_model = SVC(probability=True)):\n",
    "        self.BaseModel = base_model\n",
    "\n",
    "    #Function to build the classifier model. Iteratively boost the BaseModel by pseudolabeling.\n",
    "    def fit(self, X, y1,\n",
    "            n_neighbors=4, n_jobs = 1,\n",
    "            max_models = 15,\n",
    "            sample_percent = 0.5,\n",
    "            sigma_percentile = 90,\n",
    "            similarity_kernel = 'rbf',\n",
    "            verbose = False):\n",
    "        \n",
    "        # Convert from Dataframe to npArray\n",
    "        y = np.array(y1)\n",
    "        \n",
    "        # Localize labeled data\n",
    "        #idx_label = np.argwhere((y == labels[0]) | (y == labels[1])).flatten()\n",
    "        #idx_not_label = np.argwhere(y == unlabels[0]).flatten()\n",
    "        l = []\n",
    "        u = []\n",
    "        for i in range(y.size):\n",
    "            if y[i] in [0,1]:\n",
    "                l.append(i)\n",
    "            else:\n",
    "                u.append(i)\n",
    "        idx_label = np.array(l)\n",
    "        idx_not_label = np.array(u)\n",
    "        \n",
    "        if verbose:\n",
    "            print('There are still ', idx_not_label.shape[0], ' unlabeled observations')\n",
    "\n",
    "        # The parameter C is defined in the paper as\n",
    "        if idx_not_label.shape[0]!=0:\n",
    "            C = idx_label.shape[0]/idx_not_label.shape[0]\n",
    "        else:\n",
    "            C = float('inf')\n",
    "\n",
    "        # First we need to create the similarity matrix\n",
    "        if similarity_kernel == 'knn':\n",
    "\n",
    "            self.S = neighbors.kneighbors_graph(X,\n",
    "                                                n_neighbors=n_neighbors,\n",
    "                                                mode='distance',\n",
    "                                                include_self=True,\n",
    "                                                n_jobs=n_jobs)\n",
    "\n",
    "            self.S = sparse.csr_matrix(self.S)\n",
    "\n",
    "        elif similarity_kernel == 'rbf':\n",
    "            # First aprox\n",
    "            self.S = np.sqrt(rbf_kernel(X, gamma = 1))\n",
    "            # set gamma parameter as the 15th percentile\n",
    "            sigma = np.percentile(np.log(self.S), sigma_percentile)\n",
    "            sigma_2 = (1/sigma**2)*np.ones((self.S.shape[0],self.S.shape[0]))\n",
    "            self.S = np.power(self.S, sigma_2)\n",
    "            # Matrix to sparse\n",
    "            self.S = sparse.csr_matrix(self.S)\n",
    "\n",
    "        else:\n",
    "            print('No kernel type ', similarity_kernel)\n",
    "        \n",
    "        # If there is no unlabelled data, simply train the classifier on all of the data (supervised learning).\n",
    "        if idx_not_label.shape[0]==0:\n",
    "            self.models = []\n",
    "            clf = self.BaseModel\n",
    "            clf.fit(X, y)\n",
    "            self.models.append(clf)\n",
    "            return\n",
    "\n",
    "        #=============================================================\n",
    "        # Initialise variables\n",
    "        #=============================================================\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "        H = np.zeros(idx_not_label.shape[0])\n",
    "\n",
    "        # Loop for adding sequential models\n",
    "        for t in range(max_models):\n",
    "            \n",
    "            #=============================================================\n",
    "            # Calculate p_i and q_i for every sample\n",
    "            #=============================================================\n",
    "            p_1 = np.einsum('ij,j', self.S[:,idx_label].todense(), (y[idx_label]==1))[idx_not_label]*np.exp(-2*H)\n",
    "            p_2 = np.einsum('ij,j', self.S[:,idx_not_label].todense(), np.exp(H))[idx_not_label]*np.exp(-H)\n",
    "            p = np.add(p_1, p_2)\n",
    "            p = np.squeeze(np.asarray(p))\n",
    "\n",
    "            q_1 = np.einsum('ij,j', self.S[:,idx_label].todense(), (y[idx_label]==-1))[idx_not_label]*np.exp(2*H)\n",
    "            q_2 = np.einsum('ij,j', self.S[:,idx_not_label].todense(), np.exp(-H))[idx_not_label]*np.exp(H)\n",
    "            q = np.add(q_1, q_2)\n",
    "            q = np.squeeze(np.asarray(q))\n",
    "\n",
    "            #=============================================================\n",
    "            # Compute predicted label z_i\n",
    "            #=============================================================\n",
    "            z = np.sign(p-q)\n",
    "            z_conf = np.abs(p-q)\n",
    "            \n",
    "            #=============================================================\n",
    "            # Sample sample_percent most confident predictions\n",
    "            #=============================================================\n",
    "            # Sampling weights\n",
    "            sample_weights = z_conf/np.sum(z_conf)\n",
    "            \n",
    "            # If there are non-zero weights\n",
    "            if np.any((sample_weights != 0)):\n",
    "                if z.size != 1 :                    \n",
    "                    idx_aux = np.random.choice(np.arange(z.size), size = int(sample_percent*idx_not_label.size), p = sample_weights, replace = False)\n",
    "                else :\n",
    "                    idx_aux = [0]\n",
    "                    \n",
    "                idx_sample = idx_not_label[idx_aux]\n",
    "            else:\n",
    "                print('No similar unlabeled observations left.')\n",
    "                break\n",
    "\n",
    "            # Create new X_t, y_t\n",
    "            idx_total_sample = np.concatenate([idx_label,idx_sample]).flatten().tolist()\n",
    "            #X_t = X[idx_total_sample,]\n",
    "            X_t = X.iloc[idx_total_sample]\n",
    "            if z.size !=1:\n",
    "                np.put(y, idx_sample, z[idx_aux]) # Include predicted to train new model\n",
    "            else:\n",
    "                y[idx_sample] = z\n",
    "            y_t = y[idx_total_sample]\n",
    "\n",
    "            #=============================================================\n",
    "            # Fit BaseModel to samples using predicted labels\n",
    "            #=============================================================\n",
    "            # Fit model to unlabeled observations\n",
    "            clf = self.BaseModel\n",
    "            clf.fit(X_t, y_t)\n",
    "            # Make predictions for unlabeled observations\n",
    "            h = clf.predict(X.iloc[idx_not_label])\n",
    "\n",
    "            # Refresh indexes\n",
    "            idx_label = idx_total_sample\n",
    "            idx_not_label = np.array([i for i in np.arange(y.size) if i not in idx_label])\n",
    "                \n",
    "            # If no samples are left without label, break\n",
    "            if idx_not_label.size == 0:\n",
    "                if verbose:\n",
    "                    print('All observations have been labeled')\n",
    "                    print('Number of iterations: ', t + 1)\n",
    "                break\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('There are still ', idx_not_label.shape[0], ' unlabeled observations')\n",
    "\n",
    "            #=============================================================\n",
    "            # Compute weight (a) for the BaseModel as in (12)\n",
    "            #=============================================================\n",
    "            e = (np.dot(p,h==-1) + np.dot(q,h==1))/(np.sum(np.add(p,q)))\n",
    "            if e!=0:\n",
    "                a = 0.25*np.log((1-e)/e)\n",
    "            else:\n",
    "                e =  0.00001\n",
    "                a = 0.25*np.log((1-e)/e)\n",
    "            \n",
    "            #=============================================================\n",
    "            # Update final model\n",
    "            #=============================================================\n",
    "            # If a<0 the model is not converging\n",
    "            if a<0:\n",
    "                if verbose:\n",
    "                    print('Problematic convergence of the model. a<0')\n",
    "                break\n",
    "            elif np.isposinf(a):\n",
    "                print('Problematic convergence of the model. a=inf')\n",
    "                self.models.append(clf)\n",
    "                self.weights.append(a)\n",
    "                break\n",
    "\n",
    "            # Save model\n",
    "            self.models.append(clf)\n",
    "            #save weights\n",
    "            self.weights.append(a)\n",
    "            # Update\n",
    "            H = np.zeros(idx_not_label.size)\n",
    "            w = np.sum(self.weights)\n",
    "            for i in range(len(self.models)):\n",
    "                H = np.add(H, self.weights[i]*self.models[i].predict(X.iloc[idx_not_label]))\n",
    "                # H = np.add(H, self.weights[i]*self.models[i].predict_proba(X[idx_not_label])[:,1]/w)\n",
    "\n",
    "            # H = np.array(list(map(lambda x: 1 if x>0 else -1, H)))\n",
    "            #=============================================================\n",
    "            # Breaking conditions\n",
    "            #=============================================================\n",
    "\n",
    "            # Maximum number of models reached\n",
    "            if (t==max_models) & verbose:\n",
    "                print('Maximum number of models reached')\n",
    "\n",
    "            # If no samples are left without label, break\n",
    "            if idx_not_label.size == 0:\n",
    "                if verbose:\n",
    "                    print('All observations have been labeled')\n",
    "                    print('Number of iterations: ',t + 1)\n",
    "                break\n",
    "\n",
    "        if verbose:\n",
    "            print('\\n The model weights are \\n')\n",
    "            print(self.weights)\n",
    "\n",
    "    def predict(self, X, semi):\n",
    "        if semi:\n",
    "            estimate = np.zeros(X.shape[0])\n",
    "            # Predict weighting each model\n",
    "            w = np.sum(self.weights)\n",
    "            for i in range(len(self.models)):\n",
    "                # estimate = np.add(estimate,  self.weights[i]*self.models[i].predict_proba(X)[:,1]/w)\n",
    "                estimate = np.add(estimate, self.weights[i]*self.models[i].predict(X))\n",
    "            estimate = np.array(list(map(lambda x: 1 if x>0 else -1, estimate)))\n",
    "            estimate = estimate.astype(int)\n",
    "            return estimate\n",
    "        else:\n",
    "            return self.models[-1].predict(X)\n",
    "    \n",
    "    def predict_proba(self, X, semi):\n",
    "        if semi:\n",
    "            estimate = np.zeros(X.shape[0])\n",
    "            # Predict weighting each model\n",
    "            w = np.sum(self.weights)\n",
    "            for i in range(len(self.models)):\n",
    "                estimate = np.add(estimate,  self.weights[i]*self.models[i].predict_proba(X)[:,1]/w)\n",
    "                # estimate = np.add(estimate, self.weights[i]*self.models[i].predict(X))\n",
    "            # estimate = np.array(list(map(lambda x: 1 if x>0 else -1, estimate)))\n",
    "            # estimate = estimate.astype(int)\n",
    "            return estimate\n",
    "        else:\n",
    "            return self.models[-1].predict_proba(X)[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = 'online_shoppers_intentions'\n",
    "\n",
    "df1r10 = pd.read_csv(\"../data/train/noresampling/test.csv\", index_col=0)\n",
    "df1u10 = pd.read_csv(\"../data/train/undersampled/\"+filename1+\"_10.csv\", index_col=0)\n",
    "df1t = pd.read_csv(\"../data/test/\"+filename1+\".csv\", index_col=0)\n",
    "\n",
    "numerical_features1 = [\"Administrative\", \"Administrative_Duration\", \"Informational\", \"Informational_Duration\", \n",
    "                      \"ProductRelated\", \"ProductRelated_Duration\", \"BounceRates\", \"ExitRates\", \"PageValues\", \"SpecialDay\"]\n",
    "numerical_features1 = []\n",
    "categorical_features1 = [\"OperatingSystems\", \"Browser\", \"Region\", \"TrafficType\", \"VisitorType\", \"Weekend\", \"Month\"]\n",
    "#categorical_features1 = []\n",
    "target1 = 'Revenue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SemiBoost_Test(dftrain, dftest, categorical_features, numerical_features, target, model, semi):\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    X_train = dftrain[numerical_features+categorical_features]\n",
    "    y_train = dftrain[target]\n",
    "    pseudo = y_train.fillna(-1)\n",
    "    \n",
    "    X_test = dftest[numerical_features+categorical_features]\n",
    "    y_test = dftest[target]\n",
    "    \n",
    "    #print(pseudo.to_list())\n",
    "    \n",
    "    X_train[categorical_features] = X_train[categorical_features].astype('category')\n",
    "    X_test[categorical_features] = X_test[categorical_features].astype('category')\n",
    "    \n",
    "    model.fit(X_train, pseudo, verbose=True, similarity_kernel = 'rbf')\n",
    "    y_pred = model.predict(X_test, semi)\n",
    "    y_pred_prob = model.predict_proba(X_test, semi)\n",
    "    \n",
    "    execution = time.time() - start_time\n",
    "    \n",
    "    return [execution, y_pred, y_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity Check\n",
    "\n",
    "#[t, p, pp] = SemiBoost_Test(df1u10, df1t, categorical_features1, numerical_features1, target1, SemiBoostClassifier(RandomForestClassifier()), False)\n",
    "\n",
    "#pp\n",
    "#p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
