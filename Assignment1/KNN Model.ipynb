{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns\n",
    "from numpy import set_printoptions\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"online_shoppers_intention.csv\")\n",
    "preprocessed_df = pd.read_csv(\"preprocessedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['VisitorType_New_Visitor', 'VisitorType_Other', 'VisitorType_Returning_Visitor', 'Month_Aug', 'Month_Dec', 'Month_Feb', 'Month_Jul', 'Month_June', 'Month_Mar', 'Month_May', 'Month_Nov', 'Month_Oct', 'Month_Sep']\n",
    "target = \"Revenue\"\n",
    "\n",
    "preprocessed_df[categorical_features] = preprocessed_df[categorical_features].astype('category') \n",
    "preprocessed_df[target] = preprocessed_df[target].astype('category') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12330 entries, 0 to 12329\n",
      "Data columns (total 21 columns):\n",
      " #   Column                         Non-Null Count  Dtype   \n",
      "---  ------                         --------------  -----   \n",
      " 0   Unnamed: 0                     12330 non-null  int64   \n",
      " 1   PageValues                     12330 non-null  float64 \n",
      " 2   ExitRates                      12330 non-null  float64 \n",
      " 3   ProductRelated                 12330 non-null  float64 \n",
      " 4   Administrative                 12330 non-null  float64 \n",
      " 5   Informational                  12330 non-null  float64 \n",
      " 6   SpecialDay                     12330 non-null  float64 \n",
      " 7   Revenue                        12330 non-null  category\n",
      " 8   Month_Aug                      12330 non-null  category\n",
      " 9   Month_Dec                      12330 non-null  category\n",
      " 10  Month_Feb                      12330 non-null  category\n",
      " 11  Month_Jul                      12330 non-null  category\n",
      " 12  Month_June                     12330 non-null  category\n",
      " 13  Month_Mar                      12330 non-null  category\n",
      " 14  Month_May                      12330 non-null  category\n",
      " 15  Month_Nov                      12330 non-null  category\n",
      " 16  Month_Oct                      12330 non-null  category\n",
      " 17  Month_Sep                      12330 non-null  category\n",
      " 18  VisitorType_New_Visitor        12330 non-null  category\n",
      " 19  VisitorType_Other              12330 non-null  category\n",
      " 20  VisitorType_Returning_Visitor  12330 non-null  category\n",
      "dtypes: category(14), float64(6), int64(1)\n",
      "memory usage: 844.3 KB\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into x and y\n",
    "\n",
    "X = preprocessed_df.drop(columns=[\"Revenue\"])\n",
    "y = preprocessed_df[\"Revenue\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed:   11.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metric': 'manhattan', 'n_neighbors': 15, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "# Parameters for Hyperparameter Tuning\n",
    "grid_params = { 'n_neighbors' : [2,3,4,5,7,9,11,13,15],\n",
    "               'weights' : ['uniform','distance'],\n",
    "               'metric' : ['minkowski','euclidean','manhattan']}\n",
    "\n",
    "gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)\n",
    "g_res = gs.fit(X_train, y_train)\n",
    "g_res.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 10 KNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________Iteration:0\n",
      "Overall F1-Score                                    : 0.010274\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92      3127\n",
      "           1       0.25      0.01      0.01       572\n",
      "\n",
      "    accuracy                           0.84      3699\n",
      "   macro avg       0.55      0.50      0.46      3699\n",
      "weighted avg       0.75      0.84      0.78      3699\n",
      "\n",
      "__________________________________________________________________________________Iteration:1\n",
      "Overall F1-Score                                    : 0.006908\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92      3127\n",
      "           1       0.29      0.00      0.01       572\n",
      "\n",
      "    accuracy                           0.84      3699\n",
      "   macro avg       0.57      0.50      0.46      3699\n",
      "weighted avg       0.76      0.84      0.78      3699\n",
      "\n",
      "__________________________________________________________________________________Iteration:2\n",
      "Overall F1-Score                                    : 0.000000\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91      3127\n",
      "           1       0.00      0.00      0.00       572\n",
      "\n",
      "    accuracy                           0.84      3699\n",
      "   macro avg       0.42      0.50      0.46      3699\n",
      "weighted avg       0.71      0.84      0.77      3699\n",
      "\n",
      "__________________________________________________________________________________Iteration:3\n",
      "Overall F1-Score                                    : 0.010292\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92      3127\n",
      "           1       0.27      0.01      0.01       572\n",
      "\n",
      "    accuracy                           0.84      3699\n",
      "   macro avg       0.56      0.50      0.46      3699\n",
      "weighted avg       0.76      0.84      0.78      3699\n",
      "\n",
      "__________________________________________________________________________________Iteration:4\n",
      "Overall F1-Score                                    : 0.006814\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.91      3127\n",
      "           1       0.13      0.00      0.01       572\n",
      "\n",
      "    accuracy                           0.84      3699\n",
      "   macro avg       0.49      0.50      0.46      3699\n",
      "weighted avg       0.74      0.84      0.77      3699\n",
      "\n",
      "__________________________________________________________________________________Iteration:5\n",
      "Overall F1-Score                                    : 0.006873\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92      3127\n",
      "           1       0.20      0.00      0.01       572\n",
      "\n",
      "    accuracy                           0.84      3699\n",
      "   macro avg       0.52      0.50      0.46      3699\n",
      "weighted avg       0.75      0.84      0.77      3699\n",
      "\n",
      "__________________________________________________________________________________Iteration:6\n",
      "Overall F1-Score                                    : 0.013746\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92      3127\n",
      "           1       0.40      0.01      0.01       572\n",
      "\n",
      "    accuracy                           0.84      3699\n",
      "   macro avg       0.62      0.50      0.46      3699\n",
      "weighted avg       0.78      0.84      0.78      3699\n",
      "\n",
      "__________________________________________________________________________________Iteration:7\n",
      "Overall F1-Score                                    : 0.006885\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92      3127\n",
      "           1       0.22      0.00      0.01       572\n",
      "\n",
      "    accuracy                           0.84      3699\n",
      "   macro avg       0.53      0.50      0.46      3699\n",
      "weighted avg       0.75      0.84      0.77      3699\n",
      "\n",
      "__________________________________________________________________________________Iteration:8\n",
      "Overall F1-Score                                    : 0.010239\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.91      3127\n",
      "           1       0.21      0.01      0.01       572\n",
      "\n",
      "    accuracy                           0.84      3699\n",
      "   macro avg       0.53      0.50      0.46      3699\n",
      "weighted avg       0.75      0.84      0.77      3699\n",
      "\n",
      "__________________________________________________________________________________Iteration:9\n",
      "Overall F1-Score                                    : 0.003448\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92      3127\n",
      "           1       0.12      0.00      0.00       572\n",
      "\n",
      "    accuracy                           0.84      3699\n",
      "   macro avg       0.49      0.50      0.46      3699\n",
      "weighted avg       0.73      0.84      0.77      3699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split dataframe into train and test data\n",
    "# Note: Stratify preserves the propotion of Revenue of T/F in the testing and training sets\n",
    "\n",
    "y_predictions = []\n",
    "accuracies = []\n",
    "confusionMatrices = []\n",
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"__________________________________________________________________________________Iteration:\"+str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "    # Create KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=13, algorithm = 'brute', metric='manhattan', weights='uniform')\n",
    "\n",
    "    # Fit the classifier to the data\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Make predications on the test data\n",
    "    y_preds_knn = knn.predict(X_test)\n",
    "    y_predictions.append(y_preds_knn)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    confusionMatrices.append(confusion_matrix(y_test, y_preds_knn))\n",
    "\n",
    "    # Accuracy Scores\n",
    "    accuracies.append(accuracy_score(y_test, y_preds_knn))\n",
    "    f1_scores.append(f1_score(y_test, y_preds_knn))\n",
    "    recalls.append(recall_score(y_test, y_preds_knn))\n",
    "    precisions.append(precision_score(y_test, y_preds_knn))\n",
    "    \n",
    "    # Print some results\n",
    "    print('Overall F1-Score                                    : %.6f'%f1_score(y_test, y_preds_knn))\n",
    "    print('\\nClassification Report                       : ')\n",
    "    print(classification_report(y_test, y_preds_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.8437145174371452\n",
      "Average F1-Score: 0.007547876271452072\n",
      "Average Recall: 0.0038461538461538464\n",
      "Average Precision: 0.21032828282828284\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Accuracy: \"+str(mean(accuracies)))\n",
    "print(\"Average F1-Score: \"+str(mean(f1_scores)))\n",
    "print(\"Average Recall: \"+str(mean(recalls)))\n",
    "print(\"Average Precision: \"+str(mean(precisions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3699, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"Accuracy\":accuracies, \"F1_Score\":f1_scores, \"Recall\":recalls, \"Precision\":precisions}\n",
    "knn_df = pd.DataFrame(data)  \n",
    "knn_df.to_csv(\"knn_evaluation.csv\")\n",
    "\n",
    "data_y = {\"Y_true\":y_test}\n",
    "for i in  range(10):\n",
    "    data_y['Y_pred_'+str(i)] = y_predictions[i]\n",
    "    \n",
    "knn_df_y = pd.DataFrame(data_y) \n",
    "knn_df_y.to_csv(\"knn_predictions.csv\")\n",
    "\n",
    "knn_df_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________\n",
      "KNN Model: 0\n",
      "[[3118    9]\n",
      " [ 569    3]]\n",
      "____________________________\n",
      "KNN Model: 1\n",
      "[[3122    5]\n",
      " [ 570    2]]\n",
      "____________________________\n",
      "KNN Model: 2\n",
      "[[3118    9]\n",
      " [ 572    0]]\n",
      "____________________________\n",
      "KNN Model: 3\n",
      "[[3119    8]\n",
      " [ 569    3]]\n",
      "____________________________\n",
      "KNN Model: 4\n",
      "[[3114   13]\n",
      " [ 570    2]]\n",
      "____________________________\n",
      "KNN Model: 5\n",
      "[[3119    8]\n",
      " [ 570    2]]\n",
      "____________________________\n",
      "KNN Model: 6\n",
      "[[3121    6]\n",
      " [ 568    4]]\n",
      "____________________________\n",
      "KNN Model: 7\n",
      "[[3120    7]\n",
      " [ 570    2]]\n",
      "____________________________\n",
      "KNN Model: 8\n",
      "[[3116   11]\n",
      " [ 569    3]]\n",
      "____________________________\n",
      "KNN Model: 9\n",
      "[[3120    7]\n",
      " [ 571    1]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(confusionMatrices)):\n",
    "    print(\"____________________________\")\n",
    "    print(\"KNN Model: \"+str(i))\n",
    "    print(confusionMatrices[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random over-sampling:\n",
      "1    10422\n",
      "0    10422\n",
      "Name: Revenue, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Class count\n",
    "count_class_0, count_class_1 = preprocessed_df['Revenue'].value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = preprocessed_df[preprocessed_df['Revenue'] == 0]\n",
    "df_class_1 = preprocessed_df[preprocessed_df['Revenue'] == 1]\n",
    "\n",
    "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "\n",
    "print('Random over-sampling:')\n",
    "print(df_test_over['Revenue'].value_counts())\n",
    "\n",
    "X = df_test_over.drop(columns=[\"Revenue\"])\n",
    "y = df_test_over[\"Revenue\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________Iteration:0\n",
      "Overall F1-Score                                    : 0.713535\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.67      0.69      3127\n",
      "           1       0.69      0.74      0.71      3127\n",
      "\n",
      "    accuracy                           0.70      6254\n",
      "   macro avg       0.70      0.70      0.70      6254\n",
      "weighted avg       0.70      0.70      0.70      6254\n",
      "\n",
      "__________________________________________________________________________________Iteration:1\n",
      "Overall F1-Score                                    : 0.705991\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.66      0.68      3127\n",
      "           1       0.68      0.73      0.71      3127\n",
      "\n",
      "    accuracy                           0.70      6254\n",
      "   macro avg       0.70      0.70      0.70      6254\n",
      "weighted avg       0.70      0.70      0.70      6254\n",
      "\n",
      "__________________________________________________________________________________Iteration:2\n",
      "Overall F1-Score                                    : 0.712939\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.67      0.69      3127\n",
      "           1       0.69      0.74      0.71      3127\n",
      "\n",
      "    accuracy                           0.70      6254\n",
      "   macro avg       0.70      0.70      0.70      6254\n",
      "weighted avg       0.70      0.70      0.70      6254\n",
      "\n",
      "__________________________________________________________________________________Iteration:3\n",
      "Overall F1-Score                                    : 0.712081\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.68      0.70      3127\n",
      "           1       0.69      0.73      0.71      3127\n",
      "\n",
      "    accuracy                           0.70      6254\n",
      "   macro avg       0.71      0.70      0.70      6254\n",
      "weighted avg       0.71      0.70      0.70      6254\n",
      "\n",
      "__________________________________________________________________________________Iteration:4\n",
      "Overall F1-Score                                    : 0.707891\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.66      0.68      3127\n",
      "           1       0.68      0.74      0.71      3127\n",
      "\n",
      "    accuracy                           0.70      6254\n",
      "   macro avg       0.70      0.70      0.70      6254\n",
      "weighted avg       0.70      0.70      0.70      6254\n",
      "\n",
      "__________________________________________________________________________________Iteration:5\n",
      "Overall F1-Score                                    : 0.709163\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.65      0.68      3127\n",
      "           1       0.68      0.74      0.71      3127\n",
      "\n",
      "    accuracy                           0.70      6254\n",
      "   macro avg       0.70      0.70      0.70      6254\n",
      "weighted avg       0.70      0.70      0.70      6254\n",
      "\n",
      "__________________________________________________________________________________Iteration:6\n",
      "Overall F1-Score                                    : 0.699844\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.67      0.69      3127\n",
      "           1       0.69      0.72      0.70      3127\n",
      "\n",
      "    accuracy                           0.69      6254\n",
      "   macro avg       0.69      0.69      0.69      6254\n",
      "weighted avg       0.69      0.69      0.69      6254\n",
      "\n",
      "__________________________________________________________________________________Iteration:7\n",
      "Overall F1-Score                                    : 0.696169\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.67      0.68      3127\n",
      "           1       0.68      0.71      0.70      3127\n",
      "\n",
      "    accuracy                           0.69      6254\n",
      "   macro avg       0.69      0.69      0.69      6254\n",
      "weighted avg       0.69      0.69      0.69      6254\n",
      "\n",
      "__________________________________________________________________________________Iteration:8\n",
      "Overall F1-Score                                    : 0.709887\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.67      0.69      3127\n",
      "           1       0.69      0.73      0.71      3127\n",
      "\n",
      "    accuracy                           0.70      6254\n",
      "   macro avg       0.70      0.70      0.70      6254\n",
      "weighted avg       0.70      0.70      0.70      6254\n",
      "\n",
      "__________________________________________________________________________________Iteration:9\n",
      "Overall F1-Score                                    : 0.709638\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.66      0.69      3127\n",
      "           1       0.69      0.74      0.71      3127\n",
      "\n",
      "    accuracy                           0.70      6254\n",
      "   macro avg       0.70      0.70      0.70      6254\n",
      "weighted avg       0.70      0.70      0.70      6254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split dataframe into train and test data\n",
    "# Note: Stratify preserves the propotion of Revenue of T/F in the testing and training sets\n",
    "\n",
    "y_predictions_over = []\n",
    "accuracies_over = []\n",
    "confusionMatrices_over = []\n",
    "f1_scores_over = []\n",
    "precisions_over = []\n",
    "recalls_over = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"__________________________________________________________________________________Iteration:\"+str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "    # Create KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=13, algorithm = 'brute', metric='manhattan', weights='uniform')\n",
    "\n",
    "    # Fit the classifier to the data\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Make predications on the test data\n",
    "    y_preds_knn = knn.predict(X_test)\n",
    "    y_predictions_over.append(y_preds_knn)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    confusionMatrices_over.append(confusion_matrix(y_test, y_preds_knn))\n",
    "\n",
    "    # Accuracy Scores\n",
    "    accuracies_over.append(accuracy_score(y_test, y_preds_knn))\n",
    "    f1_scores_over.append(f1_score(y_test, y_preds_knn))\n",
    "    recalls_over.append(recall_score(y_test, y_preds_knn))\n",
    "    precisions_over.append(precision_score(y_test, y_preds_knn))\n",
    "    \n",
    "    # Print some results\n",
    "    print('Overall F1-Score                                    : %.6f'%f1_score(y_test, y_preds_knn))\n",
    "    print('\\nClassification Report                       : ')\n",
    "    print(classification_report(y_test, y_preds_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.6981451870802686\n",
      "Average F1-Score: 0.7077137774125685\n",
      "Average Recall: 0.7309561880396547\n",
      "Average Precision: 0.6859586022087348\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Accuracy: \"+str(mean(accuracies_over)))\n",
    "print(\"Average F1-Score: \"+str(mean(f1_scores_over)))\n",
    "print(\"Average Recall: \"+str(mean(recalls_over)))\n",
    "print(\"Average Precision: \"+str(mean(precisions_over)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6254, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"Accuracy\":accuracies_over, \"F1_Score\":f1_scores_over, \"Recall\":recalls_over, \"Precision\":precisions_over}\n",
    "knn_df = pd.DataFrame(data)  \n",
    "knn_df.to_csv(\"knn_evaluation_over.csv\")\n",
    "\n",
    "data_y = {\"Y_true\":y_test}\n",
    "for i in  range(10):\n",
    "    data_y['Y_pred_'+str(i)] = y_predictions_over[i]\n",
    "    \n",
    "knn_df_y = pd.DataFrame(data_y) \n",
    "knn_df_y.to_csv(\"knn_predictions_over.csv\")\n",
    "\n",
    "knn_df_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________\n",
      "KNN Model (oversampling): 0\n",
      "[[2091 1036]\n",
      " [ 818 2309]]\n",
      "____________________________\n",
      "KNN Model (oversampling): 1\n",
      "[[2064 1063]\n",
      " [ 841 2286]]\n",
      "____________________________\n",
      "KNN Model (oversampling): 2\n",
      "[[2091 1036]\n",
      " [ 821 2306]]\n",
      "____________________________\n",
      "KNN Model (oversampling): 3\n",
      "[[2123 1004]\n",
      " [ 843 2284]]\n",
      "____________________________\n",
      "KNN Model (oversampling): 4\n",
      "[[2054 1073]\n",
      " [ 826 2301]]\n",
      "____________________________\n",
      "KNN Model (oversampling): 5\n",
      "[[2042 1085]\n",
      " [ 813 2314]]\n",
      "____________________________\n",
      "KNN Model (oversampling): 6\n",
      "[[2100 1027]\n",
      " [ 891 2236]]\n",
      "____________________________\n",
      "KNN Model (oversampling): 7\n",
      "[[2085 1042]\n",
      " [ 901 2226]]\n",
      "____________________________\n",
      "KNN Model (oversampling): 8\n",
      "[[2085 1042]\n",
      " [ 833 2294]]\n",
      "____________________________\n",
      "KNN Model (oversampling): 9\n",
      "[[2070 1057]\n",
      " [ 826 2301]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(confusionMatrices_over)):\n",
    "    print(\"____________________________\")\n",
    "    print(\"KNN Model (oversampling): \"+str(i))\n",
    "    print(confusionMatrices_over[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random under-sampling:\n",
      "1    1908\n",
      "0    1908\n",
      "Name: Revenue, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Class count\n",
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "print(df_test_under['Revenue'].value_counts())\n",
    "\n",
    "\n",
    "X = df_test_under.drop(columns=[\"Revenue\"])\n",
    "y = df_test_under[\"Revenue\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________Iteration:0\n",
      "Overall F1-Score                                    : 0.566753\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.55      0.55       572\n",
      "           1       0.56      0.57      0.57       573\n",
      "\n",
      "    accuracy                           0.56      1145\n",
      "   macro avg       0.56      0.56      0.56      1145\n",
      "weighted avg       0.56      0.56      0.56      1145\n",
      "\n",
      "__________________________________________________________________________________Iteration:1\n",
      "Overall F1-Score                                    : 0.544474\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.59      0.57       572\n",
      "           1       0.56      0.53      0.54       573\n",
      "\n",
      "    accuracy                           0.56      1145\n",
      "   macro avg       0.56      0.56      0.56      1145\n",
      "weighted avg       0.56      0.56      0.56      1145\n",
      "\n",
      "__________________________________________________________________________________Iteration:2\n",
      "Overall F1-Score                                    : 0.557641\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.59      0.58       573\n",
      "           1       0.57      0.55      0.56       572\n",
      "\n",
      "    accuracy                           0.57      1145\n",
      "   macro avg       0.57      0.57      0.57      1145\n",
      "weighted avg       0.57      0.57      0.57      1145\n",
      "\n",
      "__________________________________________________________________________________Iteration:3\n",
      "Overall F1-Score                                    : 0.574449\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.59      0.58       572\n",
      "           1       0.58      0.57      0.57       573\n",
      "\n",
      "    accuracy                           0.58      1145\n",
      "   macro avg       0.58      0.58      0.58      1145\n",
      "weighted avg       0.58      0.58      0.58      1145\n",
      "\n",
      "__________________________________________________________________________________Iteration:4\n",
      "Overall F1-Score                                    : 0.547804\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.53      0.53       573\n",
      "           1       0.54      0.56      0.55       572\n",
      "\n",
      "    accuracy                           0.54      1145\n",
      "   macro avg       0.54      0.54      0.54      1145\n",
      "weighted avg       0.54      0.54      0.54      1145\n",
      "\n",
      "__________________________________________________________________________________Iteration:5\n",
      "Overall F1-Score                                    : 0.571429\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.58      0.57       573\n",
      "           1       0.57      0.57      0.57       572\n",
      "\n",
      "    accuracy                           0.57      1145\n",
      "   macro avg       0.57      0.57      0.57      1145\n",
      "weighted avg       0.57      0.57      0.57      1145\n",
      "\n",
      "__________________________________________________________________________________Iteration:6\n",
      "Overall F1-Score                                    : 0.541522\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.53      0.53       572\n",
      "           1       0.54      0.55      0.54       573\n",
      "\n",
      "    accuracy                           0.54      1145\n",
      "   macro avg       0.54      0.54      0.54      1145\n",
      "weighted avg       0.54      0.54      0.54      1145\n",
      "\n",
      "__________________________________________________________________________________Iteration:7\n",
      "Overall F1-Score                                    : 0.532513\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54       573\n",
      "           1       0.54      0.53      0.53       572\n",
      "\n",
      "    accuracy                           0.54      1145\n",
      "   macro avg       0.54      0.54      0.54      1145\n",
      "weighted avg       0.54      0.54      0.54      1145\n",
      "\n",
      "__________________________________________________________________________________Iteration:8\n",
      "Overall F1-Score                                    : 0.567521\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.54      0.55       572\n",
      "           1       0.56      0.58      0.57       573\n",
      "\n",
      "    accuracy                           0.56      1145\n",
      "   macro avg       0.56      0.56      0.56      1145\n",
      "weighted avg       0.56      0.56      0.56      1145\n",
      "\n",
      "__________________________________________________________________________________Iteration:9\n",
      "Overall F1-Score                                    : 0.566038\n",
      "\n",
      "Classification Report                       : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.54      0.55       573\n",
      "           1       0.56      0.58      0.57       572\n",
      "\n",
      "    accuracy                           0.56      1145\n",
      "   macro avg       0.56      0.56      0.56      1145\n",
      "weighted avg       0.56      0.56      0.56      1145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split dataframe into train and test data\n",
    "# Note: Stratify preserves the propotion of Revenue of T/F in the testing and training sets\n",
    "\n",
    "y_predictions_under = []\n",
    "accuracies_under = []\n",
    "confusionMatrices_under = []\n",
    "f1_scores_under = []\n",
    "precisions_under = []\n",
    "recalls_under = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"__________________________________________________________________________________Iteration:\"+str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "    # Create KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=13, algorithm = 'brute', metric='manhattan', weights='uniform')\n",
    "\n",
    "    # Fit the classifier to the data\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Make predications on the test data\n",
    "    y_preds_knn = knn.predict(X_test)\n",
    "    y_predictions_under.append(y_preds_knn)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    confusionMatrices_under.append(confusion_matrix(y_test, y_preds_knn))\n",
    "\n",
    "    # Accuracy Scores\n",
    "    accuracies_under.append(accuracy_score(y_test, y_preds_knn))\n",
    "    f1_scores_under.append(f1_score(y_test, y_preds_knn))\n",
    "    recalls_under.append(recall_score(y_test, y_preds_knn))\n",
    "    precisions_under.append(precision_score(y_test, y_preds_knn))\n",
    "    \n",
    "    # Print some results\n",
    "    print('Overall F1-Score                                    : %.6f'%f1_score(y_test, y_preds_knn))\n",
    "    print('\\nClassification Report                       : ')\n",
    "    print(classification_report(y_test, y_preds_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.5566812227074236\n",
      "Average F1-Score: 0.5570144247458094\n",
      "Average Recall: 0.5575528746994716\n",
      "Average Precision: 0.5567806476899141\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Accuracy: \"+str(mean(accuracies_under)))\n",
    "print(\"Average F1-Score: \"+str(mean(f1_scores_under)))\n",
    "print(\"Average Recall: \"+str(mean(recalls_under)))\n",
    "print(\"Average Precision: \"+str(mean(precisions_under)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1145, 11)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"Accuracy\":accuracies_under, \"F1_Score\":f1_scores_under, \"Recall\":recalls_under, \"Precision\":precisions_under}\n",
    "knn_df = pd.DataFrame(data)  \n",
    "knn_df.to_csv(\"knn_evaluation_under.csv\")\n",
    "\n",
    "data_y = {\"Y_true\":y_test}\n",
    "for i in  range(10):\n",
    "    data_y['Y_pred_'+str(i)] = y_predictions_under[i]\n",
    "    \n",
    "knn_df_y = pd.DataFrame(data_y) \n",
    "knn_df_y.to_csv(\"knn_predictions_under.csv\")\n",
    "\n",
    "knn_df_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________\n",
      "KNN Model (undersampling): 0\n",
      "[[313 259]\n",
      " [244 329]]\n",
      "____________________________\n",
      "KNN Model (undersampling): 1\n",
      "[[335 237]\n",
      " [270 303]]\n",
      "____________________________\n",
      "KNN Model (undersampling): 2\n",
      "[[338 235]\n",
      " [260 312]]\n",
      "____________________________\n",
      "KNN Model (undersampling): 3\n",
      "[[336 236]\n",
      " [247 326]]\n",
      "____________________________\n",
      "KNN Model (undersampling): 4\n",
      "[[302 271]\n",
      " [254 318]]\n",
      "____________________________\n",
      "KNN Model (undersampling): 5\n",
      "[[330 243]\n",
      " [246 326]]\n",
      "____________________________\n",
      "KNN Model (undersampling): 6\n",
      "[[302 270]\n",
      " [260 313]]\n",
      "____________________________\n",
      "KNN Model (undersampling): 7\n",
      "[[310 263]\n",
      " [269 303]]\n",
      "____________________________\n",
      "KNN Model (undersampling): 8\n",
      "[[307 265]\n",
      " [241 332]]\n",
      "____________________________\n",
      "KNN Model (undersampling): 9\n",
      "[[309 264]\n",
      " [242 330]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(confusionMatrices_under)):\n",
    "    print(\"____________________________\")\n",
    "    print(\"KNN Model (undersampling): \"+str(i))\n",
    "    print(confusionMatrices_under[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
