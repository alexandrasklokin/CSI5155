{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code based of the implementation at: https://github.com/papabloblo/semi_boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from sklearn.svm import SVC\n",
    "from scipy import sparse\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "\n",
    "class SemiBoostClassifier():\n",
    "\n",
    "    def __init__(self, base_model = SVC(probability=True)):\n",
    "        self.BaseModel = base_model\n",
    "\n",
    "    '''Fit model'''\n",
    "    def fit(self, X, y1,\n",
    "            n_neighbors=4, n_jobs = 1,\n",
    "            max_models = 15,\n",
    "            sample_percent = 0.5,\n",
    "            sigma_percentile = 90,\n",
    "            similarity_kernel = 'rbf',\n",
    "            verbose = False):\n",
    "        \n",
    "        # Convert from Dataframe to npArray\n",
    "        y = np.array(y1)\n",
    "        \n",
    "        # Localize labeled data\n",
    "        #idx_label = np.argwhere((y == labels[0]) | (y == labels[1])).flatten()\n",
    "        #idx_not_label = np.argwhere(y == unlabels[0]).flatten()\n",
    "        l = []\n",
    "        u = []\n",
    "        for i in range(y.size):\n",
    "            if y[i] in [0,1]:\n",
    "                l.append(i)\n",
    "            else:\n",
    "                u.append(i)\n",
    "        idx_label = np.array(l)\n",
    "        idx_not_label = np.array(u)\n",
    "\n",
    "        # The parameter C is defined in the paper as\n",
    "        if idx_not_label.shape[0]!=0:\n",
    "            C = idx_label.shape[0]/idx_not_label.shape[0]\n",
    "        else:\n",
    "            C = float('inf')\n",
    "\n",
    "        # First we need to create the similarity matrix\n",
    "        if similarity_kernel == 'knn':\n",
    "\n",
    "            self.S = neighbors.kneighbors_graph(X,\n",
    "                                                n_neighbors=n_neighbors,\n",
    "                                                mode='distance',\n",
    "                                                include_self=True,\n",
    "                                                n_jobs=n_jobs)\n",
    "\n",
    "            self.S = sparse.csr_matrix(self.S)\n",
    "\n",
    "        elif similarity_kernel == 'rbf':\n",
    "            # First aprox\n",
    "            self.S = np.sqrt(rbf_kernel(X, gamma = 1))\n",
    "            # set gamma parameter as the 15th percentile\n",
    "            sigma = np.percentile(np.log(self.S), sigma_percentile)\n",
    "            sigma_2 = (1/sigma**2)*np.ones((self.S.shape[0],self.S.shape[0]))\n",
    "            self.S = np.power(self.S, sigma_2)\n",
    "            # Matrix to sparse\n",
    "            self.S = sparse.csr_matrix(self.S)\n",
    "\n",
    "        else:\n",
    "            print('No kernel type ', similarity_kernel)\n",
    "            \n",
    "        if idx_not_label.shape[0]==0:\n",
    "            self.models = []\n",
    "            clf = self.BaseModel\n",
    "            clf.fit(X, y)\n",
    "            self.models.append(clf)\n",
    "            return\n",
    "\n",
    "        #=============================================================\n",
    "        # Initialise variables\n",
    "        #=============================================================\n",
    "        self.models = []\n",
    "        self.weights = []\n",
    "        H = np.zeros(idx_not_label.shape[0])\n",
    "\n",
    "        # Loop for adding sequential models\n",
    "        for t in range(max_models):\n",
    "            \n",
    "            #=============================================================\n",
    "            # Calculate p_i and q_i for every sample\n",
    "            #=============================================================\n",
    "            p_1 = np.einsum('ij,j', self.S[:,idx_label].todense(), (y[idx_label]==1))[idx_not_label]*np.exp(-2*H)\n",
    "            p_2 = np.einsum('ij,j', self.S[:,idx_not_label].todense(), np.exp(H))[idx_not_label]*np.exp(-H)\n",
    "            p = np.add(p_1, p_2)\n",
    "            p = np.squeeze(np.asarray(p))\n",
    "\n",
    "            q_1 = np.einsum('ij,j', self.S[:,idx_label].todense(), (y[idx_label]==-1))[idx_not_label]*np.exp(2*H)\n",
    "            q_2 = np.einsum('ij,j', self.S[:,idx_not_label].todense(), np.exp(-H))[idx_not_label]*np.exp(H)\n",
    "            q = np.add(q_1, q_2)\n",
    "            q = np.squeeze(np.asarray(q))\n",
    "\n",
    "            #=============================================================\n",
    "            # Compute predicted label z_i\n",
    "            #=============================================================\n",
    "            z = np.sign(p-q)\n",
    "            z_conf = np.abs(p-q)\n",
    "            \n",
    "            #=============================================================\n",
    "            # Sample sample_percent most confident predictions\n",
    "            #=============================================================\n",
    "            # Sampling weights\n",
    "            sample_weights = z_conf/np.sum(z_conf)\n",
    "            \n",
    "            # If there are non-zero weights\n",
    "            if np.any((sample_weights != 0)):\n",
    "                if z.size != 1 :\n",
    "                    idx_aux = np.random.choice(np.arange(z.size), size = int(sample_percent*idx_not_label.size), p = sample_weights, replace = False)\n",
    "                else :\n",
    "                    idx_aux = [0]\n",
    "                    \n",
    "                idx_sample = idx_not_label[idx_aux]\n",
    "            else:\n",
    "                print('No similar unlabeled observations left.')\n",
    "                break\n",
    "\n",
    "            # Create new X_t, y_t\n",
    "            idx_total_sample = np.concatenate([idx_label,idx_sample]).flatten().tolist()\n",
    "            #X_t = X[idx_total_sample,]\n",
    "            X_t = X.iloc[idx_total_sample]\n",
    "            if z.size !=1:\n",
    "                np.put(y, idx_sample, z[idx_aux]) # Include predicted to train new model\n",
    "            else:\n",
    "                #print(\"y[idx_sample]: \"+ str(y[idx_sample]))\n",
    "                y[idx_sample] = z\n",
    "            y_t = y[idx_total_sample]\n",
    "\n",
    "            #=============================================================\n",
    "            # Fit BaseModel to samples using predicted labels\n",
    "            #=============================================================\n",
    "            # Fit model to unlabeled observations\n",
    "            clf = self.BaseModel\n",
    "            clf.fit(X_t, y_t)\n",
    "            # Make predictions for unlabeled observations\n",
    "            h = clf.predict(X.iloc[idx_not_label])\n",
    "\n",
    "            # Refresh indexes\n",
    "            idx_label = idx_total_sample\n",
    "            idx_not_label = np.array([i for i in np.arange(y.size) if i not in idx_label])\n",
    "                \n",
    "            # If no samples are left without label, break\n",
    "            if idx_not_label.size == 0:\n",
    "                if verbose:\n",
    "                    print('All observations have been labeled')\n",
    "                    print('Number of iterations: ',t + 1)\n",
    "                break\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('There are still ', idx_not_label.shape[0], ' unlabeled observations')\n",
    "\n",
    "            #=============================================================\n",
    "            # Compute weight (a) for the BaseModel as in (12)\n",
    "            #=============================================================\n",
    "            e = (np.dot(p,h==-1) + np.dot(q,h==1))/(np.sum(np.add(p,q)))\n",
    "            a = 0.25*np.log((1-e)/e)\n",
    "            \n",
    "            #=============================================================\n",
    "            # Update final model\n",
    "            #=============================================================\n",
    "            # If a<0 the model is not converging\n",
    "            if a<0:\n",
    "                if verbose:\n",
    "                    print('Problematic convergence of the model. a<0')\n",
    "                break\n",
    "\n",
    "            # Save model\n",
    "            self.models.append(clf)\n",
    "            #save weights\n",
    "            self.weights.append(a)\n",
    "            # Update\n",
    "            H = np.zeros(idx_not_label.size)\n",
    "            w = np.sum(self.weights)\n",
    "            for i in range(len(self.models)):\n",
    "                H = np.add(H, self.weights[i]*self.models[i].predict(X.iloc[idx_not_label]))\n",
    "                # H = np.add(H, self.weights[i]*self.models[i].predict_proba(X[idx_not_label])[:,1]/w)\n",
    "\n",
    "            # H = np.array(list(map(lambda x: 1 if x>0 else -1, H)))\n",
    "            #=============================================================\n",
    "            # Breaking conditions\n",
    "            #=============================================================\n",
    "\n",
    "            # Maximum number of models reached\n",
    "            if (t==max_models) & verbose:\n",
    "                print('Maximum number of models reached')\n",
    "\n",
    "            # If no samples are left without label, break\n",
    "            if idx_not_label.size == 0:\n",
    "                if verbose:\n",
    "                    print('All observations have been labeled')\n",
    "                    print('Number of iterations: ',t + 1)\n",
    "                break\n",
    "\n",
    "        if verbose:\n",
    "            print('\\n The model weights are \\n')\n",
    "            print(self.weights)\n",
    "\n",
    "    def predict(self, X, semi):\n",
    "        if semi:\n",
    "            estimate = np.zeros(X.shape[0])\n",
    "            # Predict weighting each model\n",
    "            w = np.sum(self.weights)\n",
    "            for i in range(len(self.models)):\n",
    "                estimate = np.add(estimate,  self.weights[i]*self.models[i].predict_proba(X)[:,1]/w)\n",
    "                # estimate = np.add(estimate, self.weights[i]*self.models[i].predict(X))\n",
    "            estimate = np.array(list(map(lambda x: 1 if x>0 else -1, estimate)))\n",
    "            estimate = estimate.astype(int)\n",
    "            return estimate\n",
    "        else:\n",
    "            return self.models[-1].predict(X)\n",
    "    \n",
    "    def predict_proba(self, X, semi):\n",
    "        if semi:\n",
    "            estimate = np.zeros(X.shape[0])\n",
    "            # Predict weighting each model\n",
    "            w = np.sum(self.weights)\n",
    "            for i in range(len(self.models)):\n",
    "                estimate = np.add(estimate,  self.weights[i]*self.models[i].predict_proba(X)[:,1]/w)\n",
    "                # estimate = np.add(estimate, self.weights[i]*self.models[i].predict(X))\n",
    "            # estimate = np.array(list(map(lambda x: 1 if x>0 else -1, estimate)))\n",
    "            # estimate = estimate.astype(int)\n",
    "            return estimate\n",
    "        else:\n",
    "            return self.models[-1].predict_proba(X)[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename2 = 'marketing_campaign'\n",
    "\n",
    "df2r0 = pd.read_csv(\"../data/train/noresampling/\"+filename2+\"_0.csv\", index_col=0)\n",
    "df2r10 = pd.read_csv(\"../data/train/noresampling/\"+filename2+\"_10.csv\", index_col=0)\n",
    "df2r20 = pd.read_csv(\"../data/train/noresampling/\"+filename2+\"_20.csv\", index_col=0)\n",
    "df2r50 = pd.read_csv(\"../data/train/noresampling/\"+filename2+\"_50.csv\", index_col=0)\n",
    "df2r90 = pd.read_csv(\"../data/train/noresampling/\"+filename2+\"_90.csv\", index_col=0)\n",
    "df2r95 = pd.read_csv(\"../data/train/noresampling/\"+filename2+\"_95.csv\", index_col=0)\n",
    "\n",
    "df2t = pd.read_csv(\"../data/test/\"+filename2+\".csv\", index_col=0)\n",
    "\n",
    "numerical_features2 = ['Income','MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts','MntSweetProducts', \n",
    "                        'MntGoldProds','Year_Birth','Recency','NumDealsPurchases','NumWebPurchases',\n",
    "                        'NumCatalogPurchases','NumStorePurchases','NumWebVisitsMonth','Dt_Customer']\n",
    "categorical_features2 = ['Education','Marital_Status','Kidhome','AcceptedCmp3', \n",
    "                        'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2','Complain','Response']\n",
    "target2 = 'Teenhome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SemiBoost_Test(dftrain, dftest, categorical_features, numerical_features, target, model, semi):\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    X_train = dftrain[numerical_features+categorical_features]\n",
    "    y_train = dftrain[target]\n",
    "    pseudo = y_train.fillna(-1)\n",
    "    \n",
    "    X_test = dftest[numerical_features+categorical_features]\n",
    "    y_test = dftest[target]\n",
    "    \n",
    "    #print(pseudo.to_list())\n",
    "    \n",
    "    X_train[categorical_features] = X_train[categorical_features].astype('category')\n",
    "    X_test[categorical_features] = X_test[categorical_features].astype('category')\n",
    "    \n",
    "    model.fit(X_train, pseudo, verbose=True)\n",
    "    y_pred = model.predict(X_test, semi)\n",
    "    y_pred_prob = model.predict_proba(X_test, semi)\n",
    "    \n",
    "    execution = time.time() - start_time\n",
    "    \n",
    "    return [execution, y_pred, y_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\core\\frame.py:3065: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are still  78  unlabeled observations\n",
      "There are still  39  unlabeled observations\n",
      "There are still  20  unlabeled observations\n",
      "There are still  10  unlabeled observations\n",
      "There are still  5  unlabeled observations\n",
      "There are still  3  unlabeled observations\n",
      "There are still  2  unlabeled observations\n",
      "There are still  1  unlabeled observations\n",
      "All observations have been labeled\n",
      "Number of iterations:  9\n",
      "\n",
      " The model weights are \n",
      "\n",
      "[0.5802541167664359, 0.5392112052236815, 0.44367837029138363, 0.2558947023181577, 0.2780124979226367, 0.28699693165665857, 0.266309732728326, 0.2379299115058545]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.51530761, 0.54054618, 0.55251369, 0.52326889, 0.53263162,\n",
       "       0.53119059, 0.55065673, 0.54546814, 0.51671239, 0.50664431,\n",
       "       0.54331925, 0.51961713, 0.53100344, 0.59038943, 0.51046894,\n",
       "       0.52892112, 0.5199755 , 0.51156305, 0.53021107, 0.50692842,\n",
       "       0.52658174, 0.53150533, 0.53159059, 0.52675501, 0.52750565,\n",
       "       0.53130232, 0.54950391, 0.54826672, 0.55697558, 0.5287919 ,\n",
       "       0.53339935, 0.53449491, 0.53634484, 0.52710062, 0.53477151,\n",
       "       0.54826289, 0.5       , 0.53686077, 0.52909684, 0.52834037,\n",
       "       0.52053795, 0.51479064, 0.53717985, 0.51998995, 0.54964899,\n",
       "       0.53260119, 0.58504457, 0.54870365, 0.51899326, 0.53853956,\n",
       "       0.52889972, 0.57022844, 0.53371692, 0.56497019, 0.53507553,\n",
       "       0.5143968 , 0.51753181, 0.55001352, 0.53407457, 0.52277026,\n",
       "       0.57155271, 0.52147576, 0.52575933, 0.52793597, 0.56723799,\n",
       "       0.52226391, 0.5784903 , 0.53099052, 0.51670347, 0.53266294,\n",
       "       0.53507615, 0.58440743, 0.53194054, 0.53014051, 0.55143728,\n",
       "       0.51689308, 0.52267621, 0.53956282, 0.53717922, 0.52818472,\n",
       "       0.53999213, 0.51129716, 0.52554813, 0.54236988, 0.58003415,\n",
       "       0.52917608, 0.51919877, 0.52670583, 0.53043053, 0.51911864,\n",
       "       0.54044154, 0.53005487, 0.53653938, 0.52982797, 0.58361456,\n",
       "       0.52618323, 0.5166112 , 0.5460569 , 0.53553638, 0.55533915,\n",
       "       0.52355801, 0.54347042, 0.53127683, 0.52008665, 0.53546798,\n",
       "       0.53040231, 0.55637386, 0.54317724, 0.55411457, 0.51982871,\n",
       "       0.56347909, 0.5543129 , 0.53822314, 0.580383  , 0.5555794 ,\n",
       "       0.55164109, 0.52169129, 0.55665871, 0.52905932, 0.52787839,\n",
       "       0.5247806 , 0.52012591, 0.5619509 , 0.54874242, 0.555907  ,\n",
       "       0.51706891, 0.55448839, 0.52376052, 0.52873255, 0.55223492,\n",
       "       0.52213374, 0.53661879, 0.53222041, 0.51446796, 0.52663887,\n",
       "       0.51372273, 0.51942786, 0.54002327, 0.56365797, 0.55004283,\n",
       "       0.57227764, 0.51718154, 0.50805426, 0.55273898, 0.56030788,\n",
       "       0.54737493, 0.55666514, 0.5512675 , 0.5233013 , 0.54594861,\n",
       "       0.51808041, 0.53308837, 0.54066856, 0.51269912, 0.52644053,\n",
       "       0.51582161, 0.53627513, 0.53598716, 0.52202585, 0.5277071 ,\n",
       "       0.53986882, 0.50981232, 0.54253866, 0.55387524, 0.53292522,\n",
       "       0.54402626, 0.53635252, 0.57530732, 0.53409749, 0.55193629,\n",
       "       0.5081706 , 0.54490439, 0.52045609, 0.55494777, 0.51717482,\n",
       "       0.53839322, 0.53602183, 0.54693873, 0.5215843 , 0.53263162,\n",
       "       0.52229528, 0.52513726, 0.52574845, 0.51349065, 0.56180986,\n",
       "       0.54669959, 0.52711393, 0.5       , 0.51579427, 0.52001086,\n",
       "       0.51465915, 0.54890156, 0.51977179, 0.51023291, 0.53081617,\n",
       "       0.51759195, 0.55188546, 0.52583565, 0.53105701, 0.53286113,\n",
       "       0.53473321, 0.5134169 , 0.51545111, 0.54570631, 0.52312035,\n",
       "       0.53028882, 0.51222807, 0.53022394, 0.51612148, 0.51960572,\n",
       "       0.52130663, 0.52708378, 0.53867816, 0.52992576, 0.5079403 ,\n",
       "       0.52616299, 0.52303649, 0.52020066, 0.51501246, 0.5283957 ,\n",
       "       0.5215237 , 0.5343982 , 0.54009693, 0.5621211 , 0.55655002,\n",
       "       0.51730243, 0.54436838, 0.52814065, 0.5490764 , 0.53080734,\n",
       "       0.51661063, 0.52893954, 0.55079981, 0.52878026, 0.52200692,\n",
       "       0.52641235, 0.5407952 , 0.55355077, 0.53287488, 0.54604255,\n",
       "       0.55854138, 0.51456412, 0.51375385, 0.55294812, 0.5224585 ,\n",
       "       0.53492145, 0.52353393, 0.51673749, 0.53831096, 0.54402913,\n",
       "       0.52855186, 0.58621093, 0.52241447, 0.52016415, 0.5262276 ,\n",
       "       0.5325254 , 0.54641396, 0.5593688 , 0.51160239, 0.53857964,\n",
       "       0.54743607, 0.5334791 , 0.55251207, 0.53064391, 0.52360339,\n",
       "       0.53830914, 0.56297616, 0.54398576, 0.57030877, 0.54524276,\n",
       "       0.54724269, 0.52927055, 0.53433104, 0.56484556, 0.50755467,\n",
       "       0.5054906 , 0.5       , 0.52588976, 0.52215251, 0.54580686,\n",
       "       0.55362173, 0.53216909, 0.5189299 , 0.52469068, 0.52289618,\n",
       "       0.51637764, 0.53352083, 0.53807375, 0.51481895, 0.51931223,\n",
       "       0.52566518, 0.52492199, 0.55251529, 0.54565385, 0.53032056,\n",
       "       0.51317163, 0.53436293, 0.52550751, 0.53402272, 0.53022915,\n",
       "       0.5471952 , 0.54432483, 0.52217157, 0.53335308, 0.52818179,\n",
       "       0.51474113, 0.55753388, 0.52322483, 0.55304708, 0.51265793,\n",
       "       0.55834819, 0.53161122, 0.52224213, 0.53449491, 0.55031588,\n",
       "       0.51782353, 0.54416876, 0.54139715, 0.53966856, 0.55220886,\n",
       "       0.5268765 , 0.51086714, 0.54526818, 0.52331079, 0.5119175 ,\n",
       "       0.52625887, 0.5108824 , 0.53188026, 0.52146542, 0.55166736,\n",
       "       0.52208391, 0.53486513, 0.54362168, 0.54078722, 0.51749636,\n",
       "       0.51235234, 0.53688895, 0.51747734, 0.54127457, 0.53594831,\n",
       "       0.52701016, 0.52488301, 0.51338032, 0.51922359, 0.52454774,\n",
       "       0.51031554, 0.51596407, 0.52205335, 0.55299626, 0.59398497,\n",
       "       0.50625247, 0.59316968, 0.54095224, 0.51872013, 0.51613437,\n",
       "       0.54061154, 0.54724628, 0.52735291, 0.51885384, 0.51208603,\n",
       "       0.52377152, 0.52665973, 0.54329631, 0.51739789, 0.57497399,\n",
       "       0.52863866, 0.53373518, 0.53628045, 0.52995729, 0.55565277,\n",
       "       0.54392118, 0.52446982, 0.54293875, 0.53023438, 0.51626842,\n",
       "       0.55533755, 0.53533557, 0.53049778, 0.54519592, 0.51744171,\n",
       "       0.56129887, 0.51751372, 0.52261333, 0.5678428 , 0.51860442,\n",
       "       0.52889417, 0.54308464, 0.51565014, 0.51625928, 0.51638293,\n",
       "       0.55105789, 0.57416039, 0.55457393, 0.51558548, 0.56189785,\n",
       "       0.52169723, 0.52072872, 0.52046368, 0.5       , 0.53895005,\n",
       "       0.55150858, 0.55096193, 0.57717458, 0.56103223, 0.5480183 ,\n",
       "       0.54231303, 0.54113113, 0.56067168, 0.53632695, 0.52826378,\n",
       "       0.51871741, 0.54927592, 0.57835434, 0.5244689 , 0.50901238,\n",
       "       0.51809044, 0.52465211, 0.51961142, 0.54379959, 0.52893808,\n",
       "       0.51340839, 0.53601793, 0.53632749, 0.52957372, 0.5247806 ,\n",
       "       0.51223169, 0.52008201, 0.5268765 , 0.53292504, 0.56676517,\n",
       "       0.53523663, 0.56153174, 0.53600334, 0.53754407, 0.54397334,\n",
       "       0.54254273, 0.55833558, 0.53382177, 0.5139896 , 0.52469068,\n",
       "       0.5347868 , 0.5356583 , 0.51643082, 0.54939333, 0.52680197,\n",
       "       0.56189785, 0.51042548, 0.5183153 , 0.53050513, 0.52927083,\n",
       "       0.52664954, 0.53235848, 0.53550109, 0.53744339, 0.52059981,\n",
       "       0.55006466, 0.54373906, 0.52177513, 0.51946652, 0.53576702,\n",
       "       0.54675901, 0.51357932, 0.53885019, 0.53111063, 0.52335704,\n",
       "       0.53558118, 0.56320928, 0.53301549, 0.53565339, 0.58129072,\n",
       "       0.51459841, 0.53246175, 0.53966856, 0.56033212, 0.52130684,\n",
       "       0.54285836, 0.53140432, 0.51371803, 0.53683175, 0.5701467 ,\n",
       "       0.54013006, 0.5278231 , 0.5376759 , 0.5404199 , 0.55807616,\n",
       "       0.5439337 , 0.52970124, 0.52771357, 0.53082812, 0.57046982,\n",
       "       0.51222807, 0.52091012, 0.52243424, 0.51121929, 0.5478688 ,\n",
       "       0.53091234, 0.51555773, 0.52460879, 0.52205206, 0.51642613,\n",
       "       0.57775443, 0.5617696 , 0.51536521, 0.52531265, 0.52423051,\n",
       "       0.53299329, 0.52379474, 0.51776831, 0.56796213, 0.5       ,\n",
       "       0.52220698, 0.5537924 , 0.60121526, 0.57279866, 0.5140283 ,\n",
       "       0.53281822, 0.53614041, 0.50504419, 0.56634239, 0.54687449,\n",
       "       0.56261913, 0.53775888, 0.5238621 , 0.51388239, 0.52118623,\n",
       "       0.51573393, 0.53583831, 0.54247787, 0.54034453, 0.55164109,\n",
       "       0.55775168, 0.55870497, 0.54023713, 0.55408717, 0.53821525,\n",
       "       0.52699742, 0.53563425, 0.56321469, 0.54826672, 0.54756817,\n",
       "       0.5       , 0.5325254 , 0.53070744, 0.52188055, 0.56308122,\n",
       "       0.54622273, 0.55251207, 0.5269515 , 0.56082066, 0.54114484,\n",
       "       0.53100871, 0.56469725, 0.54848963, 0.55496021, 0.55618319,\n",
       "       0.51672135, 0.56161689, 0.52136088, 0.55202195, 0.5164574 ,\n",
       "       0.541238  , 0.53382409, 0.55365925, 0.51085748, 0.54637978,\n",
       "       0.51485001, 0.54437271, 0.52335546, 0.55474204, 0.5784903 ,\n",
       "       0.53411256, 0.52015609, 0.54382742, 0.5304996 , 0.53474763,\n",
       "       0.52675542, 0.53028968, 0.54290122, 0.52217031, 0.50663879,\n",
       "       0.56240055, 0.55563485, 0.54146592, 0.54145902, 0.55515242,\n",
       "       0.56159427, 0.54375009, 0.56321469, 0.55824257, 0.55137366,\n",
       "       0.52987632, 0.53539686, 0.51125515, 0.53389669, 0.52932429,\n",
       "       0.556023  , 0.57979647, 0.53310611, 0.5704882 , 0.52573173,\n",
       "       0.5857383 , 0.54484657, 0.52220089, 0.55117221, 0.52665283,\n",
       "       0.52228136, 0.54521677, 0.54706201, 0.5426837 , 0.51058794,\n",
       "       0.54174407, 0.51350113, 0.51552875, 0.52436356, 0.53565339,\n",
       "       0.53991783, 0.53315504, 0.51391671, 0.52437762, 0.56365797,\n",
       "       0.51151342, 0.57511963, 0.59546197, 0.52992417, 0.54122169,\n",
       "       0.52949625, 0.54672289, 0.54041965, 0.53389669, 0.52599784,\n",
       "       0.53776786, 0.54174621, 0.54019172, 0.52499521, 0.53902638,\n",
       "       0.53416631, 0.51450515, 0.53225081, 0.53846681, 0.53965426,\n",
       "       0.52923478, 0.51964092, 0.52801099, 0.52763367, 0.50740688,\n",
       "       0.52550706, 0.55758452, 0.54589616, 0.53077986, 0.54252543,\n",
       "       0.53041191, 0.56659296, 0.52609751, 0.52822654, 0.54544518,\n",
       "       0.53407199, 0.552279  , 0.57717458, 0.53879745, 0.53866891,\n",
       "       0.51641548, 0.51135795, 0.57578016, 0.53166097, 0.52314756,\n",
       "       0.54986796, 0.53376798, 0.54104371, 0.53666351, 0.52003526,\n",
       "       0.52734885, 0.52958532])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "\n",
    "#[t, p, pp] = SemiBoost_Test(df2r10, df2t, categorical_features2, numerical_features2, target2, SemiBoostClassifier(), False)\n",
    "\n",
    "#pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
