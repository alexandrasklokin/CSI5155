{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns\n",
    "from numpy import set_printoptions\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = pd.read_csv(\"../preprocessing/preprocessedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['VisitorType_New_Visitor', 'VisitorType_Other', 'VisitorType_Returning_Visitor', 'Month_Aug', 'Month_Dec', 'Month_Feb', 'Month_Jul', 'Month_June', 'Month_Mar', 'Month_May', 'Month_Nov', 'Month_Oct', 'Month_Sep']\n",
    "target = \"Revenue\"\n",
    "\n",
    "preprocessed_df[categorical_features] = preprocessed_df[categorical_features].astype('category') \n",
    "preprocessed_df[target] = preprocessed_df[target].astype('category') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12330 entries, 0 to 12329\n",
      "Data columns (total 21 columns):\n",
      " #   Column                         Non-Null Count  Dtype   \n",
      "---  ------                         --------------  -----   \n",
      " 0   Unnamed: 0                     12330 non-null  int64   \n",
      " 1   PageValues                     12330 non-null  float64 \n",
      " 2   ExitRates                      12330 non-null  float64 \n",
      " 3   ProductRelated                 12330 non-null  float64 \n",
      " 4   Administrative                 12330 non-null  float64 \n",
      " 5   Informational                  12330 non-null  float64 \n",
      " 6   SpecialDay                     12330 non-null  float64 \n",
      " 7   Revenue                        12330 non-null  category\n",
      " 8   Month_Aug                      12330 non-null  category\n",
      " 9   Month_Dec                      12330 non-null  category\n",
      " 10  Month_Feb                      12330 non-null  category\n",
      " 11  Month_Jul                      12330 non-null  category\n",
      " 12  Month_June                     12330 non-null  category\n",
      " 13  Month_Mar                      12330 non-null  category\n",
      " 14  Month_May                      12330 non-null  category\n",
      " 15  Month_Nov                      12330 non-null  category\n",
      " 16  Month_Oct                      12330 non-null  category\n",
      " 17  Month_Sep                      12330 non-null  category\n",
      " 18  VisitorType_New_Visitor        12330 non-null  category\n",
      " 19  VisitorType_Other              12330 non-null  category\n",
      " 20  VisitorType_Returning_Visitor  12330 non-null  category\n",
      "dtypes: category(14), float64(6), int64(1)\n",
      "memory usage: 844.3 KB\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe into x and y\n",
    "\n",
    "X = preprocessed_df.drop(columns=[\"Revenue\"])\n",
    "y = preprocessed_df[\"Revenue\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 10 KNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________Iteration:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [3699, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-87ac049e73a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# AUC-PR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mprecision_knn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall_knn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0maucpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecall_knn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_knn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36mprecision_recall_curve\u001b[1;34m(y_true, probas_pred, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \"\"\"\n\u001b[1;32m--> 675\u001b[1;33m     fps, tps, thresholds = _binary_clf_curve(y_true, probas_pred,\n\u001b[0m\u001b[0;32m    676\u001b[0m                                              \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m                                              sample_weight=sample_weight)\n",
      "\u001b[1;32mc:\\users\\alexa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0} format is not supported\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3699, 1]"
     ]
    }
   ],
   "source": [
    "# Split dataframe into train and test data\n",
    "# Note: Stratify preserves the propotion of Revenue of T/F in the testing and training sets\n",
    "\n",
    "f1_scores_macro = []\n",
    "recalls_macro = []\n",
    "precisions_macro = []\n",
    "f1_scores_weighted = []\n",
    "recalls_weighted = []\n",
    "precisions_weighted = []\n",
    "accuracies = []\n",
    "auroc = []\n",
    "aucpr = []\n",
    "\n",
    "y_preds = []\n",
    "y_pred_probs = []\n",
    "y_trues = []\n",
    "\n",
    "X_tests = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"__________________________________________________________________________________Iteration:\"+str(i))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "    y_trues.append(y_test)\n",
    "\n",
    "    # Create KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=13, algorithm = 'brute', metric='manhattan', weights='uniform')\n",
    "\n",
    "    # Fit the classifier to the data\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Make predications on the test data\n",
    "    y_pred = knn.predict(X_test)\n",
    "    y_preds.append(y_pred)\n",
    "    y_pred_prob = knn.predict_proba(X_test)[:,1]\n",
    "    y_pred_probs.append(y_pred_prob)\n",
    "    \n",
    "    # Scores\n",
    "    f1_scores_macro.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    recalls_macro.append(recall_score(y_test, y_pred, average='macro'))\n",
    "    precisions_macro.append(precision_score(y_test, y_pred, average='macro'))\n",
    "    f1_scores_weighted.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    recalls_weighted.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "    precisions_weighted.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    # AUC-ROC\n",
    "    auroc.append(roc_auc_score(y_test, y_pred_prob))\n",
    "    \n",
    "    # AUC-PR\n",
    "    precision_knn, recall_knn, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "    aucpr.append(auc(recall_knn, precision_knn))\n",
    "    \n",
    "    # Print some results\n",
    "    print('\\nClassification Report                       : ')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Accuracy: \"+str(mean(accuracies)))\n",
    "print(\"Average F1-Score (macro): \"+str(mean(f1_scores_macro)))\n",
    "print(\"Average F1-Score (weighted): \"+str(mean(f1_scores_weighted)))\n",
    "print(\"Average Recall (macro): \"+str(mean(recalls_macro)))\n",
    "print(\"Average Recall (weighted): \"+str(mean(recalls_weighted)))\n",
    "print(\"Average Precision (macro): \"+str(mean(precisions_macro)))\n",
    "print(\"Average Precision (weighted): \"+str(mean(precisions_weighted)))\n",
    "print(\"Average AUROC: \"+str(mean(auroc)))\n",
    "print(\"Average AUC-PR: \"+str(mean(aucpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model\n",
    "\n",
    "# Draw confusion matrix with the best F1-Score\n",
    "best_model_index = f1_scores_weighted.index(max(f1_scores_weighted))\n",
    "print(best_model_index)\n",
    "y_true = y_trues[best_model_index]\n",
    "y_pred = y_preds[best_model_index]\n",
    "y_pred_prob = y_pred_probs[best_model_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw confusion matrix with the best F1-Score\n",
    "\n",
    "# Confusion Matrix\n",
    "cf_matrix = (confusion_matrix(y_true, y_pred))\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cf_matrix, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('KNN Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['False', 'True']); ax.yaxis.set_ticklabels(['False', 'True']);\n",
    "\n",
    "plt.savefig('../confusionMatrices/KNN_confusionMatrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = {\"Y_true\":y_true, \"Y_Pred\": y_pred, \"Y_Pred_Prob\": y_pred_prob}\n",
    "\n",
    "knn_df_y = pd.DataFrame(data_y) \n",
    "knn_df_y.to_csv(\"../predictions/knn_predictions.csv\")\n",
    "\n",
    "knn_df_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
