{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "System:\n",
      "    python: 3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)]\n",
      "executable: c:\\users\\alexa\\appdata\\local\\programs\\python\\python38\\python.exe\n",
      "   machine: Windows-10-10.0.19041-SP0\n",
      "\n",
      "Python dependencies:\n",
      "          pip: 21.3.1\n",
      "   setuptools: 47.1.0\n",
      "      sklearn: 0.24.1\n",
      "        numpy: 1.19.2\n",
      "        scipy: 1.5.2\n",
      "       Cython: 0.29.22\n",
      "       pandas: 1.1.3\n",
      "   matplotlib: 3.3.2\n",
      "       joblib: 0.17.0\n",
      "threadpoolctl: 2.1.0\n",
      "\n",
      "Built with OpenMP: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import ipynb\n",
    "\n",
    "sklearn.show_versions()\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.SemiBoost import SemiBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = 'online_shoppers_intentions'\n",
    "\n",
    "df1r0 = pd.read_csv(\"../data/train/noresampling/\"+filename1+\"_0.csv\", index_col=0)\n",
    "df1u0 = pd.read_csv(\"../data/train/undersampled/\"+filename1+\"_0.csv\", index_col=0)\n",
    "df1o0 = pd.read_csv(\"../data/train/oversampled/\"+filename1+\"_0.csv\", index_col=0)\n",
    "\n",
    "df1r10 = pd.read_csv(\"../data/train/noresampling/\"+filename1+\"_10.csv\", index_col=0)\n",
    "df1u10 = pd.read_csv(\"../data/train/undersampled/\"+filename1+\"_10.csv\", index_col=0)\n",
    "df1o10 = pd.read_csv(\"../data/train/oversampled/\"+filename1+\"_10.csv\", index_col=0)\n",
    "\n",
    "df1r20 = pd.read_csv(\"../data/train/noresampling/\"+filename1+\"_20.csv\", index_col=0)\n",
    "df1u20 = pd.read_csv(\"../data/train/undersampled/\"+filename1+\"_20.csv\", index_col=0)\n",
    "df1o20 = pd.read_csv(\"../data/train/oversampled/\"+filename1+\"_20.csv\", index_col=0)\n",
    "\n",
    "df1r50 = pd.read_csv(\"../data/train/noresampling/\"+filename1+\"_50.csv\", index_col=0)\n",
    "df1u50 = pd.read_csv(\"../data/train/undersampled/\"+filename1+\"_50.csv\", index_col=0)\n",
    "df1o50 = pd.read_csv(\"../data/train/oversampled/\"+filename1+\"_50.csv\", index_col=0)\n",
    "\n",
    "df1r90 = pd.read_csv(\"../data/train/noresampling/\"+filename1+\"_90.csv\", index_col=0)\n",
    "df1u90 = pd.read_csv(\"../data/train/undersampled/\"+filename1+\"_90.csv\", index_col=0)\n",
    "df1o90 = pd.read_csv(\"../data/train/oversampled/\"+filename1+\"_90.csv\", index_col=0)\n",
    "\n",
    "df1r95 = pd.read_csv(\"../data/train/noresampling/\"+filename1+\"_95.csv\", index_col=0)\n",
    "df1u95 = pd.read_csv(\"../data/train/undersampled/\"+filename1+\"_95.csv\", index_col=0)\n",
    "df1o95 = pd.read_csv(\"../data/train/oversampled/\"+filename1+\"_95.csv\", index_col=0)\n",
    "\n",
    "df1t = pd.read_csv(\"../data/test/\"+filename1+\".csv\", index_col=0)\n",
    "\n",
    "numerical_features1 = [\"Administrative\", \"Administrative_Duration\", \"Informational\", \"Informational_Duration\", \n",
    "                      \"ProductRelated\", \"ProductRelated_Duration\", \"BounceRates\", \"ExitRates\", \"PageValues\", \"SpecialDay\"]\n",
    "categorical_features1 = [\"OperatingSystems\", \"Browser\", \"Region\", \"TrafficType\", \"VisitorType\", \"Weekend\", \"Month\"]\n",
    "target1 = 'Revenue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename2 = 'marketing_campaign'\n",
    "\n",
    "df2r0 = pd.read_csv(\"../data/train/noresampling/\"+filename2+\"_0.csv\", index_col=0)\n",
    "df2u0 = pd.read_csv(\"../data/train/undersampled/\"+filename2+\"_0.csv\", index_col=0)\n",
    "df2o0 = pd.read_csv(\"../data/train/oversampled/\"+filename2+\"_0.csv\", index_col=0)\n",
    "\n",
    "df2r10 = pd.read_csv(\"../data/train/noresampling/\"+filename2+\"_10.csv\", index_col=0)\n",
    "df2u10 = pd.read_csv(\"../data/train/undersampled/\"+filename2+\"_10.csv\", index_col=0)\n",
    "df2o10 = pd.read_csv(\"../data/train/oversampled/\"+filename2+\"_10.csv\", index_col=0)\n",
    "\n",
    "df2r20 = pd.read_csv(\"../data/train/noresampling/\"+filename2+\"_20.csv\", index_col=0)\n",
    "df2u20 = pd.read_csv(\"../data/train/undersampled/\"+filename2+\"_20.csv\", index_col=0)\n",
    "df2o20 = pd.read_csv(\"../data/train/oversampled/\"+filename2+\"_20.csv\", index_col=0)\n",
    "\n",
    "df2r50 = pd.read_csv(\"../data/train/noresampling/\"+filename2+\"_50.csv\", index_col=0)\n",
    "df2u50 = pd.read_csv(\"../data/train/undersampled/\"+filename2+\"_50.csv\", index_col=0)\n",
    "df2o50 = pd.read_csv(\"../data/train/oversampled/\"+filename2+\"_50.csv\", index_col=0)\n",
    "\n",
    "df2r90 = pd.read_csv(\"../data/train/noresampling/\"+filename2+\"_90.csv\", index_col=0)\n",
    "df2u90 = pd.read_csv(\"../data/train/undersampled/\"+filename2+\"_90.csv\", index_col=0)\n",
    "df2o90 = pd.read_csv(\"../data/train/oversampled/\"+filename2+\"_90.csv\", index_col=0)\n",
    "\n",
    "df2r95 = pd.read_csv(\"../data/train/noresampling/\"+filename2+\"_95.csv\", index_col=0)\n",
    "df2u95 = pd.read_csv(\"../data/train/undersampled/\"+filename2+\"_95.csv\", index_col=0)\n",
    "df2o95 = pd.read_csv(\"../data/train/oversampled/\"+filename2+\"_95.csv\", index_col=0)\n",
    "\n",
    "df2t = pd.read_csv(\"../data/test/\"+filename2+\".csv\", index_col=0)\n",
    "\n",
    "numerical_features2 = ['Income','MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts','MntSweetProducts', \n",
    "                        'MntGoldProds','Year_Birth','Recency','NumDealsPurchases','NumWebPurchases',\n",
    "                        'NumCatalogPurchases','NumStorePurchases','NumWebVisitsMonth','Dt_Customer']\n",
    "categorical_features2 = ['Education','Marital_Status','Kidhome','AcceptedCmp3', \n",
    "                        'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2','Complain','Response']\n",
    "target2 = 'Teenhome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename3 = 'heart'\n",
    "\n",
    "df3r0 = pd.read_csv(\"../data/train/noresampling/\"+filename3+\"_0.csv\", index_col=0)\n",
    "df3u0 = pd.read_csv(\"../data/train/undersampled/\"+filename3+\"_0.csv\", index_col=0)\n",
    "df3o0 = pd.read_csv(\"../data/train/oversampled/\"+filename3+\"_0.csv\", index_col=0)\n",
    "\n",
    "df3r10 = pd.read_csv(\"../data/train/noresampling/\"+filename3+\"_10.csv\", index_col=0)\n",
    "df3u10 = pd.read_csv(\"../data/train/undersampled/\"+filename3+\"_10.csv\", index_col=0)\n",
    "df3o10 = pd.read_csv(\"../data/train/oversampled/\"+filename3+\"_10.csv\", index_col=0)\n",
    "\n",
    "df3r20 = pd.read_csv(\"../data/train/noresampling/\"+filename3+\"_20.csv\", index_col=0)\n",
    "df3u20 = pd.read_csv(\"../data/train/undersampled/\"+filename3+\"_20.csv\", index_col=0)\n",
    "df3o20 = pd.read_csv(\"../data/train/oversampled/\"+filename3+\"_20.csv\", index_col=0)\n",
    "\n",
    "df3r50 = pd.read_csv(\"../data/train/noresampling/\"+filename3+\"_50.csv\", index_col=0)\n",
    "df3u50 = pd.read_csv(\"../data/train/undersampled/\"+filename3+\"_50.csv\", index_col=0)\n",
    "df3o50 = pd.read_csv(\"../data/train/oversampled/\"+filename3+\"_50.csv\", index_col=0)\n",
    "\n",
    "df3r90 = pd.read_csv(\"../data/train/noresampling/\"+filename3+\"_90.csv\", index_col=0)\n",
    "df3u90 = pd.read_csv(\"../data/train/undersampled/\"+filename3+\"_90.csv\", index_col=0)\n",
    "df3o90 = pd.read_csv(\"../data/train/oversampled/\"+filename3+\"_90.csv\", index_col=0)\n",
    "\n",
    "df3r95 = pd.read_csv(\"../data/train/noresampling/\"+filename3+\"_95.csv\", index_col=0)\n",
    "df3u95 = pd.read_csv(\"../data/train/undersampled/\"+filename3+\"_95.csv\", index_col=0)\n",
    "df3o95 = pd.read_csv(\"../data/train/oversampled/\"+filename3+\"_95.csv\", index_col=0)\n",
    "\n",
    "df3t = pd.read_csv(\"../data/test/\"+filename3+\".csv\", index_col=0)\n",
    "\n",
    "numerical_features3 = ['trestbps','chol','thalach','oldpeak', 'age']\n",
    "categorical_features3 = ['sex', 'cp','fbs','restecg','exang','slope','ca','thal']\n",
    "\n",
    "target3 = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 1514\n",
      "0.9499478623566214 8631\n",
      "0.2 14590\n",
      "0.8962264150943396 212\n",
      "0.09973579920739763 1514\n",
      "0.9484536082474226 194\n",
      "0.0 91\n"
     ]
    }
   ],
   "source": [
    "# SANITY CHECKS\n",
    "print(df2u50[target2].isnull().sum()/len(df2u50), len(df2u50))\n",
    "print(df1r95[target1].isnull().sum()/len(df1r95), len(df1r95))\n",
    "print(df1o20[target1].isnull().sum()/len(df1o20), len(df1o20))\n",
    "print(df3r90[target3].isnull().sum()/len(df3r90), len(df3r90))\n",
    "print(df2u10[target2].isnull().sum()/len(df2u10), len(df2u10))\n",
    "print(df3u95[target3].isnull().sum()/len(df3u95), len(df3u95))\n",
    "print(df3t[target3].isnull().sum()/len(df3t), len(df3t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SemiBoost_1(dftrain, dftest, categorical_features, numerical_features, target, model, semi):\n",
    "        \n",
    "    start_time = time.time()\n",
    "    \n",
    "    X_train = dftrain[numerical_features+categorical_features]\n",
    "    y_train = dftrain[target]\n",
    "    pseudo = y_train.fillna(-1)\n",
    "    \n",
    "    X_test = dftest[numerical_features+categorical_features]\n",
    "    y_test = dftest[target]\n",
    "    \n",
    "    #print(pseudo.to_list())\n",
    "    \n",
    "    X_train[categorical_features] = X_train[categorical_features].astype('category')\n",
    "    X_test[categorical_features] = X_test[categorical_features].astype('category')\n",
    "    \n",
    "    model.fit(X_train, pseudo, verbose=False)\n",
    "    y_pred = model.predict(X_test, semi)\n",
    "    y_pred_prob = model.predict_proba(X_test, semi)\n",
    "    \n",
    "    execution = time.time() - start_time\n",
    "    \n",
    "    return [execution, y_pred, y_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SemiBoost_All(dftrains, dftest, categorical_features, numerical_features, target, model, sampling, filename):\n",
    "    \n",
    "    y_test = dftest[target]\n",
    "    \n",
    "    results0 = SemiBoost_1(dftrains[0], dftest, categorical_features, numerical_features, target, model, False)\n",
    "    print('DONE: '+sampling+\"-\"+filename+'-0')\n",
    "    print(\"F1-Score: \"+str(f1_score(y_test, results0[1],average='weighted')))\n",
    "    results10 = SemiBoost_1(dftrains[1], dftest, categorical_features, numerical_features, target, model, True)\n",
    "    print('DONE: '+sampling+\"-\"+filename+'-10')\n",
    "    print(\"F1-Score: \"+str(f1_score(y_test, results10[1],average='weighted')))\n",
    "    results20 = SemiBoost_1(dftrains[2], dftest, categorical_features, numerical_features, target, model, True)\n",
    "    print('DONE: '+sampling+\"-\"+filename+'-20')\n",
    "    print(\"F1-Score: \"+str(f1_score(y_test, results20[1],average='weighted')))\n",
    "    results50 = SemiBoost_1(dftrains[3], dftest, categorical_features, numerical_features, target, model, True)\n",
    "    print('DONE: '+sampling+\"-\"+filename+'-50')\n",
    "    print(\"F1-Score: \"+str(f1_score(y_test, results50[1],average='weighted')))\n",
    "    results90 = SemiBoost_1(dftrains[4], dftest, categorical_features, numerical_features, target, model, True)\n",
    "    print('DONE: '+sampling+\"-\"+filename+'-90')\n",
    "    print(\"F1-Score: \"+str(f1_score(y_test, results90[1],average='weighted')))\n",
    "    results95 = SemiBoost_1(dftrains[5], dftest, categorical_features, numerical_features, target, model, True)\n",
    "    print('DONE: '+sampling+\"-\"+filename+'-95')\n",
    "    print(\"F1-Score: \"+str(f1_score(y_test, results95[1],average='weighted')))\n",
    "    \n",
    "    executions = [results0[0],results10[0],results20[0],results50[0],results90[0],results95[0]] \n",
    "    unlabelled = ['0','10','20','50','90','95']\n",
    "    \n",
    "    predictions_data = {'y_true':y_test, 'y_0':results0[1], 'y_10':results10[1], 'y_20':results20[1], \n",
    "                        'y_50':results50[1], 'y_90':results90[1], 'y_95':results95[1], 'y_0_prob':results0[2],\n",
    "                       'y_10_prob':results10[2], 'y_20_prob':results20[2],'y_50_prob':results50[2],\n",
    "                       'y_90_prob':results90[2], 'y_95_prob':results95[2]}\n",
    "    prediction_df = pd.DataFrame(predictions_data)\n",
    "    prediction_df.to_csv('../pred/semiboost/'+sampling+'/'+filename+'.csv')\n",
    "    \n",
    "    time_data = {'unlabelled':unlabelled,'time':executions}\n",
    "    time_df = pd.DataFrame(time_data)\n",
    "    time_df.to_csv('../pred/semiboost/'+sampling+'/'+filename+'_time.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: noresampling-online_shoppers_intentions-0\n",
      "DONE: noresampling-online_shoppers_intentions-10\n",
      "DONE: noresampling-online_shoppers_intentions-20\n",
      "DONE: noresampling-online_shoppers_intentions-50\n",
      "DONE: noresampling-online_shoppers_intentions-90\n",
      "DONE: noresampling-online_shoppers_intentions-95\n",
      "DONE: undersampled-online_shoppers_intentions-0\n",
      "DONE: undersampled-online_shoppers_intentions-10\n",
      "DONE: undersampled-online_shoppers_intentions-20\n",
      "DONE: undersampled-online_shoppers_intentions-50\n",
      "DONE: undersampled-online_shoppers_intentions-90\n",
      "DONE: undersampled-online_shoppers_intentions-95\n",
      "DONE: oversampled-online_shoppers_intentions-0\n",
      "DONE: oversampled-online_shoppers_intentions-10\n",
      "DONE: oversampled-online_shoppers_intentions-20\n",
      "DONE: oversampled-online_shoppers_intentions-50\n",
      "DONE: oversampled-online_shoppers_intentions-90\n",
      "DONE: oversampled-online_shoppers_intentions-95\n"
     ]
    }
   ],
   "source": [
    "# ONLINE SHOPPING INTENTION\n",
    "sb_1 = SemiBoostClassifier(RandomForestClassifier())\n",
    "# Model does not converge with Decision Tree, nor. SVM !!\n",
    "\n",
    "noresampling = [df1r0,df1r10,df1r20,df1r50,df1r90,df1r95]\n",
    "undersampling = [df1u0,df1u10,df1u20,df1u50,df1u90,df1u95]\n",
    "oversampling = [df1o0,df1o10,df1o20,df1o50,df1o90,df1o95]\n",
    "\n",
    "SemiBoost_All(noresampling, df1t, categorical_features1, numerical_features1, target1, sb_1, 'noresampling', filename1)\n",
    "SemiBoost_All(undersampling, df1t, categorical_features1, numerical_features1, target1, sb_1, 'undersampled', filename1)\n",
    "SemiBoost_All(oversampling, df1t, categorical_features1, numerical_features1, target1, sb_1, 'oversampled', filename1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: noresampling-marketing_campaign-0\n",
      "DONE: noresampling-marketing_campaign-10\n",
      "DONE: noresampling-marketing_campaign-20\n",
      "DONE: noresampling-marketing_campaign-50\n",
      "DONE: noresampling-marketing_campaign-90\n",
      "DONE: noresampling-marketing_campaign-95\n",
      "DONE: undersampled-marketing_campaign-0\n",
      "DONE: undersampled-marketing_campaign-10\n",
      "DONE: undersampled-marketing_campaign-20\n",
      "DONE: undersampled-marketing_campaign-50\n",
      "DONE: undersampled-marketing_campaign-90\n",
      "DONE: undersampled-marketing_campaign-95\n",
      "DONE: oversampled-marketing_campaign-0\n",
      "DONE: oversampled-marketing_campaign-10\n",
      "DONE: oversampled-marketing_campaign-20\n",
      "DONE: oversampled-marketing_campaign-50\n",
      "DONE: oversampled-marketing_campaign-90\n",
      "DONE: oversampled-marketing_campaign-95\n"
     ]
    }
   ],
   "source": [
    "# MARKETING CAMPAIGN\n",
    "sb_2 = SemiBoostClassifier(RandomForestClassifier())\n",
    " \n",
    "noresampling = [df2r0,df2r10,df2r20,df2r50,df2r90,df2r95]\n",
    "undersampling = [df2u0,df2u10,df2u20,df2u50,df2u90,df2u95]\n",
    "oversampling = [df2o0,df2o10,df2o20,df2o50,df2o90,df2o95]\n",
    "\n",
    "SemiBoost_All(noresampling, df2t, categorical_features2, numerical_features2, target2, sb_2, 'noresampling', filename2)\n",
    "SemiBoost_All(undersampling, df2t, categorical_features2, numerical_features2, target2, sb_2, 'undersampled', filename2)\n",
    "SemiBoost_All(oversampling, df2t, categorical_features2, numerical_features2, target2, sb_2, 'oversampled', filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: noresampling-heart-0\n",
      "DONE: noresampling-heart-10\n",
      "DONE: noresampling-heart-20\n",
      "DONE: noresampling-heart-50\n",
      "DONE: noresampling-heart-90\n",
      "DONE: noresampling-heart-95\n",
      "DONE: undersampled-heart-0\n",
      "DONE: undersampled-heart-10\n",
      "DONE: undersampled-heart-20\n",
      "DONE: undersampled-heart-50\n",
      "DONE: undersampled-heart-90\n",
      "DONE: undersampled-heart-95\n",
      "DONE: oversampled-heart-0\n",
      "DONE: oversampled-heart-10\n",
      "DONE: oversampled-heart-20\n",
      "DONE: oversampled-heart-50\n",
      "DONE: oversampled-heart-90\n",
      "DONE: oversampled-heart-95\n"
     ]
    }
   ],
   "source": [
    "# HEART\n",
    "sb_3 = SemiBoostClassifier(RandomForestClassifier())\n",
    "\n",
    "noresampling = [df3r0,df3r10,df3r20,df3r50,df3r90,df3r95]\n",
    "undersampling = [df3u0,df3u10,df3u20,df3u50,df3u90,df3u95]\n",
    "oversampling = [df3o0,df3o10,df3o20,df3o50,df3o90,df3o95]\n",
    "\n",
    "SemiBoost_All(noresampling, df3t, categorical_features3, numerical_features3, target3, sb_3, 'noresampling', filename3)\n",
    "SemiBoost_All(undersampling, df3t, categorical_features3, numerical_features3, target3, sb_3, 'undersampled', filename3)\n",
    "SemiBoost_All(oversampling, df3t, categorical_features3, numerical_features3, target3, sb_3, 'oversampled', filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
